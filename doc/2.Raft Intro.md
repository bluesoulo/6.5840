### 1.为什么需要Raft

##### 1.1 多副本系统举例

&#8226; MapReduce复制了计算，但是复制这个动作，或者说整个MapReduce被一个单主节点控制；

&#8226; GFS以主备（primary-backup）的方式复制数据。它会实际的复制文件内容。但是它也依赖一个单主节点，来确定每一份数据的主拷贝的位置；

它们都是一个多副本系统（replication system），但是在背后，它们存在一个共性：它们需要一个单节点来决定，在多个副本中，谁是主（Primary）

你可以认为我们前面介绍的这些系统，它们将系统容错的关键点，转移到了这个单点上。这个单点，会在系统出现局部故障时，选择数据的主拷贝来继续工作。使用单点的原因是，我们需要避免脑裂（Split-Brain）。当出现故障时，我们之所以要极其小心的决定数据的主拷贝，是因为，如果不这么做的话，我们可能需要面临脑裂的场景。

##### 1.2 脑裂举例

​        假设你有一个由三个节点组成的分布式数据库集群（节点A、节点B、节点C），它们之间通过网络互相通信，并且数据在这些节点之间复制以保证一致性。某一天，由于网络故障，节点A与节点B、C之间的通信中断，但节点B和节点C仍然可以互相通信。于是，网络分区形成了**子集1**：节点A，**子集2**：节点B、节点C。

​		假设某个客户端连接到节点A并发送了一条写请求，这个请求只会更新节点A的数据，因为节点A无法与节点B、C通信。另一个客户端连接到节点B或节点C，并发送了另一条写请求，这个请求会在节点B和节点C之间复制，但无法同步到节点A。结果是，节点A的数据与节点B和节点C的数据产生不一致。

##### 1.3 一致性协议解决脑裂问题

​		使用如Raft协议的分布式一致性协议。即使在网络分区情况下，只有被选为领导者的节点（在这种情况下可能是B或C）能够处理写请求，从而保证数据一致性。

##### 1.4 过半票决

​		如果服务器的数量是奇数的，那么当出现一个网络分割时，**两个网络分区将不再对称**，假设出现了一个网络分割，那么一个分区会有两个服务器，另一个分区只会有一个服务器，这样就不再是对称的了。

​		**<u>有关过半票决系统的一个特性就是，最多只有一个网络分区会有过半的服务器，所以我们不可能有两个分区可以同时完成操作。</u>**

**在任何时候为了完成任何操作，你必须凑够过半的服务器来批准相应的操作**。

​		如果你总是需要过半的服务器才能完成任何操作，同时你有一系列的操作需要完成，其中的每一个操作都需要过半的服务器来批准，例如选举Raft的Leader，那么每一个操作对应的过半服务器，必然至少包含一个服务器存在于上一个操作的过半服务器中。也就是说，**任意两组过半服务器，至少有一个服务器是重叠的。**

​		例如，当一个Raft Leader竞选成功，那么这个Leader必然凑够了过半服务器的选票，而这组过半服务器中，必然与旧Leader的过半服务器有重叠。所以，新的Leader必然知道旧Leader使用的任期号（term number），因为新Leader的过半服务器必然与旧Leader的过半服务器有重叠，而旧Leader的过半服务器中的每一个必然都知道旧Leader的任期号。类似的，任何旧Leader提交的操作，必然存在于过半的Raft服务器中，而任何新Leader的过半服务器中，必然有至少一个服务器包含了旧Leader的所有操作。这是Raft能正确运行的一个重要因素。

### 2.Raft Log

##### 2.1 Raft Log

​        Log 是Raft Leader用来对操作排序的手段，对于复制机而言，所有副本不仅要执行相同的操作，还需要用相同的顺序执行这些操作。比如说，我有10个客户端同时向Leader发出请求，Leader必须对这些请求确定一个顺序，并确保所有其他的副本都遵从这个顺序。实际上，**Log是一些按照数字编号的槽位（类似一个数组），槽位的数字表示了Leader选择的顺序。**

​		Log的另一个用途是，在一个（非Leader，也就是Follower）副本收到了操作，但是还没有执行操作时。该副本需要将这个操作存放在某处，直到收到了Leader发送的新的commit号才执行。所以，对于Raft的Follower来说，Log是用来存放临时操作的地方。Follower收到了这些临时的操作，但是还不确定这些操作是否被commit了。我们将会看到，这些操作可能会被丢弃。

​		Log的另一个用途是用在Leader节点，我（Robert教授）很喜欢这个特性。Leader需要在它的Log中记录操作，因为这些操作可能需要重传给Follower。如果一些Follower由于网络原因或者其他原因短时间离线了或者丢了一些消息，Leader需要能够向Follower重传丢失的Log消息。所以，Leader也需要一个地方来存放客户端请求的拷贝。即使对那些已经commit的请求，为了能够向丢失了相应操作的副本重传，也需要存储在Leader的Log中。

##### 2.2. Raft Log时序

![image-20240805203158465](C:\Users\48176\AppData\Roaming\Typora\typora-user-images\image-20240805203158465.png)

​										C1为client端，S1、S2、S3为副本Server，其中S1为Leader，S2、S3为Follower

​		C1发送请求到S1，S1收到请求后写日志，然后发送AppendEntries消息到S2和S3，S2和S3收到消息后写入日志并发送确认到S1。如果S1收到超过一半服务器的确认那么会将这么指令发送到状态机，状态机执行该指令，并将执行结果返回给C1。S1会将确认消息夹带到下次发送到S1和S2的AppendEntries中（AppendEntries中存储最大Commit Id），告知Follower这两条指令已经被执行！那么S2和S3也将能在状态机中执行这条指令。

### 3.Raft选举

##### 3.1 Raft Leader

​		通常情况下，如果服务器不出现故障，有一个Leader的存在，会使得整个系统更加高效。因为有了一个大家都知道的指定Leader，对于一个请求，你可以只通过一轮消息就获得过半服务器的认可。对于一个无Leader的系统，通常需要一轮消息来确认一个临时的Leader，之后第二轮消息才能确认请求。所以，使用一个Leader可以提升系统性能至2倍。同时，有一个Leader可以更好的理解Raft系统是如何工作的。

​		Raft生命周期中可能会有不同的Leader，**它使用任期号（term number）来区分不同的Leader**。Followers（非Leader副本节点）不需要知道Leader的ID，它们只需要知道当前的任期号。**每一个任期最多有一个Leader**，这是一个很关键的特性。对于每个任期来说，或许没有Leader，或许有一个Leader，但是不可能有两个Leader出现在同一个任期中。每个任期必然最多只有一个Leader。

​		每个Raft节点都有一个**选举定时器（Election Timer）**，如果在这个定时器时间耗尽之前，当前节点没有收到任何当前Leader的消息，这个节点会认为Leader已经下线，并开始一次选举。

##### 3.2 Leader选举

​		开始一次选举时，当前服务器会增加任期号（term number），因为它想成为一个新的Leader（一个任期内不能有超过一个Leader，所以为了成为一个新的Leader，这里需要开启一个新的任期）。 之后，当前服务器会发出请求投票（RequestVote）RPC，这个消息会发给所有的Raft节点。其实只需要发送到N-1个节点，因为Raft规定了，Leader的候选人总是会在选举时投票给自己。

​		为了能够当选，**Raft要求一个候选人从过半服务器中获得认可投票**。每个Raft节点，只会在一个任期内投出一个认可选票。这意味着，***在任意一个任期内，每一个节点只会对一个候选人投一次票。***这样，<u>就不可能有两个候选人同时获得过半的选票</u>，因为每个节点只会投票一次。所以这里是过半原则导致了最多只能有一个胜出的候选人，这样我们在每个任期会有最多一个选举出的候选人。

##### 3.3选举定时器

​		如果一次选举选出了0个Leader，这次选举就失败了。Raft所有环节都在正常工作，没有故障，没有丢包，但是候选人们几乎是同时参加竞选，它们**分割了选票**（Split Vote）。没有一个节点获得了过半投票，所以也就没有人能被选上。接下来它们的选举定时器会重新计时，因为**选举定时器只会在收到了AppendEntries消息时重置**，但是由于没有Leader，所有也就没有AppendEntries消息。所有的选举定时器重新开始计时，如果我们不够幸运的话，所有的定时器又会在同一时间到期，所有节点又会投票给自己，又没有人获得了过半投票，这个状态可能会一直持续下去。

​		解决这个问题的方法是**为它们的选举定时器设置随机的超时时间**。总会有一个选举定时器先超时，而另一个后超时。假设S2和S3之间的差距足够大，先超时的那个节点（也就是S2）能够在另一个节点（也就是S3）超时之前，发起一轮选举，并获得过半的选票，那么那个节点（也就是S2）就可以成为新的Leader。

​		同时也需要注意：**选举定时器的超时时间需要至少大于Leader的心跳间隔**，否则该节点会在收到正常的心跳之前触发选举。**不同节点的选举定时器的超时时间差（S2和S3之间）必须要足够长**，使得第一个开始选举的节点能够完成一轮选举。这里至少需要大于发送一条RPC所需要的往返（Round-Trip）时间。

##### 3.5选举约束

###### 3.5.1 为什么不选择拥有最长Log的候选人作为Leader？



###### 3.5.2 Raft选举约束

​        在处理别节点发来的RequestVote RPC时，需要做一些检查才能投出赞成票。这里的限制是，**节点只能向满足下面条件之一**的候选人投出赞成票：

1. 候选人最后一条Log条目的任期号**大于**本地最后一条Log条目的任期号；
2. 候选人最后一条Log条目的任期号**等于**本地最后一条Log条目的任期号，且候选人的Log记录长度**大于等于**本地Log记录的长度；

### 4.日志恢复

##### 4.1 AppendEntries请求

​       Raft的Leader中保存了nextIndex数组，其中nextIndex[i]表示当前Raft节点下一次将要发送给第i个Follower从洗标nextIndex[i]开始的日志，使用的是AppendEntries RPC，RPC请求的参数中还会保存nextIndex[i]条日志的前一条任期prevTerm。Follwer收到AppendEntries 请求后，首先会判断prevTerm，如果该log index位置上的任期一致，那么就保存这些日志，并返回结果；如果不一致则返回失败给Leader。

​		Leader收到AppendEntries请求的结果后，如果请求成功，那么会更新nextIndex[i]；如果请求失败，那么会对nextIndex[i]减一，重新发送AppendEntries请求到第i个Follower，直到成功！

##### 4.2 快恢复

​		主要思想：让Follower返回足够的信息给Leader，**这样Leader可以以任期（Term）为单位来回退，**而不用每次只回退一条Log条目。

​		让Follower在回复Leader的AppendEntries消息中，携带3个额外的信息，来加速日志的恢复。这里的回复是指，Follower因为Log信息不匹配，拒绝了Leader的AppendEntries之后的回复。这里的三个信息是指：

		1. XTerm：这个是Follower中与Leader冲突的Log对应的任期号。
		2. XIndex：这个是Follower中，对应任期号为XTerm的第一条Log条目的槽位号。
		3. XLen：如果Follower在对应位置没有Log，那么XTerm会返回-1，XLen表示空白的Log槽位数。

​		***如何进行快速回退？***

**情况1**：XLen > 0, 则Leader回退XLen长度的日志。

**情况2**：XLen==0,判断Leader在Xindex处的Term与XTerm是否相同？如果不同则设置nextIndex[i]为Xindex，如果相同可以进一步延申判断从XIndex到nextIndex[i]之间哪一log index处Term不同，则将nextIndex[i]设置为该位置！

### 5 持久化

##### 5.1 Log

​		Log需要被持久化存储的原因是，这是唯一记录了应用程序状态的地方。如果状态机重启后，保存在内存中的状态就会被重置，那么我们就需要根据持久化在磁盘中的Log恢复状态机

##### 5.2 currentTerm和votedFor

​		currentTerm和votedFor都是用来确保每个任期只有最多一个Leader。

​		在一个故障的场景中，如果一个服务器收到了一个RequestVote请求，并且为服务器1投票了，之后它故障。如果它没有存储它为哪个服务器投过票，当它故障重启之后，收到了来自服务器2的同一个任期的另一个RequestVote请求，那么它还是会投票给服务器2，因为它发现自己的votedFor是空的，因此它认为自己还没投过票。现在这个服务器，在同一个任期内同时为服务器1和服务器2投了票。因为服务器1和服务器2都会为自己投票，它们都会认为自己有过半选票（3票中的2票），那它们都会成为Leader。现在同一个任期里面有了两个Leader。这就是为什么votedFor必须被持久化存储。

##### 5.3 持久化时机

   	 这些数据需要在每次你修改它们的时候存储起来。所以可以确定的是，安全的做法是每次你添加一个Log条目，更新currentTerm或者更新votedFor，你或许都需要持久化存储这些数据。

### 6 日志快照

​		Raft在运行过程中不断在产生Log，这些Log需要占用大量的磁盘空间去存储，如果一个服务器重启了，它需要通过重新从头开始执行这数百万条Log来重建自己的状态。为了应对这种场景，Raft有了快照（Snapshots）的概念。快照背后的思想是，要求应用程序将其状态的拷贝作为一种特殊的Log条目存储下来。

​		**对于大多数的应用程序来说，应用程序的状态远小于Log的大小**。某种程度上我们知道，在某些时间点，Log和应用程序的状态是可以互换的，它们是用来表示应用程序状态的不同事物。但是Log可能包含大量的重复的记录（例如对于X的重复赋值），这些记录使用了Log中的大量的空间，但是同时却压缩到了key-value表单中的一条记录。这在多副本系统中很常见。在这里，如果存储Log，可能尺寸会非常大，相应的，如果存储key-value表单，这可能比Log尺寸小得多。这就是快照的背后原理。

